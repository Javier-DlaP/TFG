\chapter{Sistemas de percepción con LiDAR basados en Deep Learning}
\label{cha:sistemas_de_percepcion_con_lidar_basados_en_deep_learning}

\begin{FraseCelebre}
  \begin{Frase}
    Si no conozco una cosa, la investigaré.
  \end{Frase}
  \begin{Fuente}
    Louis Pasteur
  \end{Fuente}
\end{FraseCelebre}

\noindent
Los sistemas de percepción pertenecientes al estado del arte o \ac{sota} se encuentran basados en \acs{dl}, esto no es diferente en los sistemas de percepción basados en \acs{lidar}, por lo que en este capítulo se presentan los datasets disponibles para el entrenamiento y evaluación de los modelos, las diferentes arquitecturas \acs{sota} para detección con \acs{lidar} y la herramienta utilizada para la evaluación, entrenamiento y pruebas realizadas sobre los modelos.

\section{Principales datasets}
\label{sec:principales_datasets}

Para el desarrollo de un modelo de percepción basado en \acs{dl}, es siempre necesario un conjunto de datos anotados o dataset sobre el que un modelo pueda aprender a partir de estos. En los últimos años muchas compañías han lanzado datasets para poder entrenar y validar sus modelos, además de que se muchos de estos datasets han sido publicados Open-Source para fomentar el desarrollo de nuevas técnicas.\par
Entre los datasets para conducción autónoma encontramos: A2D2 Dataset \cite{a2d2_dataset}, Argoverse Dataset \cite{argoverse_dataset}, CityScapes Dataset \cite{cityscapes_dataset}, KITTI Vision Benchmark Suite \cite{kitti_dataset}, Level 5 Open Data \cite{level_5_dataset}, nuScenes Dataset \cite{nuscenes_dataset} o Waymo Open Dataset \cite{waymo_dataset} entre otros.\par
En este apartado se van a presentar tres de los datasets más importante en la industria del automóvil, como son KITTI, nuScenes y Waymo dataset.

\subsection{KITTI}
\label{sec:kitti}

La suite de evaluación de KITTI \cite{kitti_dataset} es un sistema de evaluación para vehículos autónomos, donde se ha desarrollado una plataforma de referencia para tareas de visión estereoscópica, flujo óptico, odometría visual/\acs{slam} y detección de objetos 3D.\par
Este dataset fue presentado en 2012 en conjunto por el \acs{kit} y el \acs{ttic}, iniciando lo que años después despertaría un interés en la creación de datasets Open-Source para sistemas \acs{adas}/\acs{ads}. Al ser el primer dataset con reconocimiento internacional, fijó las bases de lo que sería el futuro de los sistemas de evaluación, además que ha sido desde su salida uno de los sistemas de evaluación más estandarizados en el desarrollo de técnicas de conducción autónoma, como se puede ver en sus más de 7.000 citas.

\begin{figure}[H]
	\begin{forest}
	for tree={
    	font=\ttfamily,
        grow'=0,
        child anchor=west,
        parent anchor=south,
        anchor=west,
        calign=first,
        inner xsep=7pt,
        edge path={
        	\noexpand\path [draw, \forestoption{edge}]
          	(!u.south west) +(7.5pt,0) |- (.child anchor) pic {folder} \forestoption{edge label};
        },
        % style for your file node 
        file/.style={edge path={\noexpand\path [draw, \forestoption{edge}]
          	(!u.south west) +(7.5pt,0) |- (.child anchor) \forestoption{edge label};},
        	inner xsep=2pt,font=\small\ttfamily
                     },
        before typesetting nodes={
        	if n=1
            {insert before={[,phantom]}}
            {}
        },
        fit=band,
        before computing xy={l=15pt},
	}  
    [KITTI Dataset
      [2011\_09\_26
        [2011\_09\_26\_drive\_0001\_sync
          [image\_00
            [data
              [0000000000.txt,file
              ]
              [0000000???.txt,file
              ]
            ]
            [timestamps.txt,file
            ]
          ]
          [image\_01
            [...,file
    	  	]
          ]
          [image\_02
            [...,file
    	  	]
          ]
          [image\_03
            [...,file
    	  	]
          ]
          [oxts
            [data
              [0000000000.txt,file
              ]
              [0000000???.txt,file
              ]
            ]
            [dataformat.txt,file
            ]
            [timestamps.txt,file
            ]
          ]
          [velodyne\_points
            [data
              [0000000000.bin,file
              ]
              [0000000???.bin,file
              ]
            ]
            [timestamps.txt,file
            ]
            [timestamps\_end.txt,file
            ]
            [timestamps\_start.txt,file
            ]
          ]
    	]
    	[2011\_09\_26\_drive\_0???\_sync
    	  [...,file
    	  ]
    	]
    	[...,file
    	]
      ]
      [20??\_??\_??
      	[...,file
      	]
      ]
      [...,file
      ]
    ]
	\end{forest}
\caption{Estructura del dataset de KITTI.}
\label{for:Estructura_del_dataset_kitti}
\end{figure}

\newpage

El sistema de percepción del vehículo utilizado para la grabación de los datos se encuentra compuesto de:
\begin{itemize}
	\item 2 sistemas de cámaras estéreo de una resolución de 1240 x 376 píxeles.
	\item 1 \acs{lidar} Velodyne HDL-64E capaz de generar más de un millón de puntos por segundo gracias a sus 64 haces láser.
	\item 1 sistema de localización \acs{sota} compuesto por \acs{gps}, \acs{gnss}, \acs{imu} y corrección de señales \acs{rtk}.
\end{itemize}

\begin{figure}[H]
	\begin{minipage}{0.48\textwidth}
		\centering
		\includegraphics[width=1\linewidth]{figures/4_sistemas_de_percepcion_con_lidar_basados_en_deep_learning/kittis_car.png}
		\caption{Vehículo utilizado en KITTI.}
		\label{fig:Vehiculo_utilizado_en_kitti}
	\end{minipage}\hfill
	\begin{minipage}{0.48\textwidth}
		\centering
		\includegraphics[width=1\linewidth]{figures/4_sistemas_de_percepcion_con_lidar_basados_en_deep_learning/kittis_car_measures.png}
		\caption{Medidas del vehículo utilizado en KITTI}
		\label{fig:Medidas_del_vehiculo_utilizado_en_kiti}
	\end{minipage}
\end{figure}

La estructura del dataset es tal y como se muestra en la figura \ref{for:Estructura_del_dataset_kitti}. Dentro de este los datos se dividen según el día de grabación y la escena, y dentro de cada escena se tienen las imágenes de la cámara estéreo monocromática (image\_00, image\_01) y RGB (image\_02, image\_03), información de todo el sistema de locación del vehículo (oxts) y las nubes de puntos provenientes del \acs{lidar} (velodyne\_points).\par
En 2020 tras el éxito que ha sido este dataset se anuncia y se abre al uso KITTI-360 \cite{kitti_360_dataset} la evolución del dataset KITTI, el cual trae mejoras añadiendo dos cámaras de ojo de pez para una obtención de visión 2D en todos los ángulos del vehículo, y una unidad láser adicional SICK. Además se incluyen en el dataset bounding boxes 3D de todo el entorno, anotaciones de instancia a partir de la información del \acs{lidar} y anotaciones de confianza sobre el vídeo tomado por las cámaras, entre otras.

\subsubsection{Análisis de la estructura del GT y las PCLs de KITTI}
\label{sec:analisis_de_la_estructura_del_gt_y_las_pcls_de_kitti}

Al estar basado este trabajo en la detección utilizando \acs{lidar}, se estudia la forma en la que se guardan los datos de las nubes de puntos junto con la representación del ground truth, para ello se analiza tanto la carpeta de velodyne\_points como el archivo tracklets.xml.\par
Para el trabajo con bounding boxes 3D, es posible utilizar unicamente el kit de detecciones 3D que contiene un conjunto de txt con la información de los objetos por cada barrido. La información de estos archivos contiene la información de: tipo, truncado, ocluido, bounding box 2D, dimensiones y localización, toda esta información se encuentra de la misma manera en el archivo tracklets.xml, pero la información de rotación se encuentra divida en un ángulo alpha y otro rotation\_y, en vez de rotación por cada uno de los tres ejes, de estos se estudiará más en profundidad en el capítulo \ref{cha:ad_devkit}.\par
Se utiliza el archivo tracklets.xml ya define el ground truth de la escena pero no por cada barrido del \acs{lidar}, sino que se guarda por cada escenario, lo que se traduce en un estructura se puede utilizar tanto para detección como para seguimiento de los objetos, todo ello como un archivo con formato \acs{xml}.\par
Se ha decido crear un programa que lea la nube de puntos indicada y que marque en un entorno 3D donde se encuentran los objetos de la escena. Esto podría ser realizado mediante el devkit que KITTI ofrece, pero este se encuentra unicamente de forma oficial en Matlab, por lo que como los modelos de \acs{dl} a utilizar se encuentran utilizando Pytorch, se deberían de analizar los datos utilizando Python.\par
La figura \ref{for:Estructura_del_archivo_tracket_labels_xml} muestra la estructura de los principales atributos del archivo tracklet\_labels.xml analizado, en este encontramos el número de los diferentes objetos del escenario, tras esto se analiza cada objeto a través de los diferentes barridos, mostrando primero las características de los objetos que se mantienen en el tiempo, como las dimensiones o el tipo de objeto y tras esto se guarda la información de posición, rotación, etc.

\begin{figure}[H]
	\begin{forest}
	for tree={
    	font=\ttfamily,
        grow'=0,
        child anchor=west,
        parent anchor=south,
        anchor=west,
        calign=first,
        inner xsep=7pt,
        edge path={
        	\noexpand\path [draw, \forestoption{edge}]
          	(!u.south west) +(7.5pt,0) |- (.child anchor) pic {folder} \forestoption{edge label};
        },
        % style for your file node 
        file/.style={edge path={\noexpand\path [draw, \forestoption{edge}]
          	(!u.south west) +(7.5pt,0) |- (.child anchor) \forestoption{edge label};},
        	inner xsep=2pt,font=\small\ttfamily
                     },
        before typesetting nodes={
        	if n=1
            {insert before={[,phantom]}}
            {}
        },
        fit=band,
        before computing xy={l=15pt},
	}  
    [tracklet\_labels.xml
      [tracklets,file
        [count,file
        ]
        [item,file
          [objectType,file
          ]
          [h,file
          ]
          [w,file
          ]
          [l,file
          ]
          [first\_frame,file
          ]
          [poses,file
            [count,file
            ]
            [item,file
              [tx,file
              ]
              [ty,file
              ]
              [tz,file
              ]
              [rx,file
              ]
              [ry,file
              ]
              [rz,file
              ]
              [state,file
              ]
              [occlusion,file
              ]
              [truncation,file
              ]
            ]
            [...,file]
          ]
        ]
        [...,file
        ]
      ]
    ]
	\end{forest}
\caption{Estructura del archivo tracklet\_labels.xml.}
\label{for:Estructura_del_archivo_tracket_labels_xml}
\end{figure}

Tras dicho análisis, utilizando la librería MayaVi para la visualización del entorno 3D, NumPy para las transformaciones en el espacio y Beautiful Soup para la lectura del \acs{xml}, se visualizan múltiples escenas del dataset.\par
Cabe destacar que como KITTI está realizado para que la parte de percepción se trabaje principalmente con la cámaras frontales del vehículo, aún teniendo información por los laterales y la parte trasera gracias al \acs{lidar}, esta no se tiene en cuenta en el ground truth, razón por la que en la imagen \ref{fig:Visualizacion_de_una_nube_de_puntos_de_kitti_junto_con_su_ground_truth} los vehículos de la parte trasera no se muestra.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.55\textwidth]{figures/4_sistemas_de_percepcion_con_lidar_basados_en_deep_learning/display_kitti_example.png}
	\caption{Visualización de una nube de puntos de KITTI junto con su ground truth.}	\label{fig:Visualizacion_de_una_nube_de_puntos_de_kitti_junto_con_su_ground_truth}
\end{figure}

Todo el código utilizado para la visualización de los objetos junto con unos escenarios de ejemplo son accesibles en: \url{https://github.com/Javier-DlaP/Display_kitti_pcl_annotations} 

\subsection{Waymo}
\label{sec:waymo}

Waymo Open Dataset \cite{waymo_dataset} es el dataset liberado de forma Open-Source por parte de Waymo y Google para la aceleración del desarrollo de tecnologías de conducción autónoma.\par
La propuesta de este dataset es la oferta de un gran número de anotaciones de alta calidad tanto 2D como 3D,  que además contienen información de seguimiento. Se han utilizado múltiples ciudades para sus escenario grabados, como son: San Francisco, Mountain View, Los Angeles, Detroit, Seattle y Phoenix. Además se trabaja con multitud de entornos y condiciones ambientales como son: construcciones, atardeceres, noches, días lluviosos, etc.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.6\textwidth]{figures/4_sistemas_de_percepcion_con_lidar_basados_en_deep_learning/waymos_car.png}
	\caption{Vehículo utilizado en Waymo.}
	\label{fig:Vehículo_utilizado_en_waymo}
\end{figure}

\newpage

Actualmente el dataset ofrece 1.950 escenas \cite{waymo_web_page} que fueron aumentadas de las 1150 escenas en la salida de la primera revisión del paper, de 20 segundos cada una,
con información recogida a 10 Hz proveniente de los sensores, lo que implican
390.000 frames. Toda esta información es recabada de:
\begin{itemize}
	\item 1 \acs{lidar} de medio alcance.
	\item 4 \acs{lidar} de corto alcance.
	\item 5 cámaras alrededor del vehículo.
\end{itemize}
Como se ve en la figura \ref{fig:Vehículo_utilizado_en_waymo}, el acercamiento de la compañía entorno a la construcción del vehículo no utiliza \acs{radar}, depende unicamente de cámaras y \acs{lidar} para el apartado de percepción. Por lo que no se puede trabajar con \acs{radar} en el dataset, además de que no se tiene información de ningún sistema de localización, ya que dicho dataset se encuentra especializado en tareas de percepción y seguimiento de los objetos de la escena.\par
Es importante tener en cuenta que en este dataset, la nube de puntos procedente del \acs{lidar} no utiliza un sistema de coordenadas cartesiano, sino un sistema de coordenadas esférico, donde las coordenadas (x, y, z) son reemplazadas por (distancia, azimuth, inclinación).

\begin{center}
$distancia = \sqrt{x²+y²+z²}$\\[10pt]
$azimuth = \atantwo(y, x)$\\[10pt]
$inclinación = \atantwo(z, \sqrt{x²+y²})$
\end{center}

Mientras que en otros datasets se tienen multitud de clases, muchas de ellas indistinguibles unas de otras, Waymo utiliza unicamente 4 tipos de objetos diferentes. En las 11,8 millones de bounding boxes 2D se encuentran vehículos, peatones y ciclistas, mientras que en las 12,6 millones de bounding boxes 3D se añaden además la detección y seguimiento de señales.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.6\textwidth]{figures/4_sistemas_de_percepcion_con_lidar_basados_en_deep_learning/classes_waymo.png}
	\caption{Nube de puntos con las diferentes clases del dataset de Waymo.}
	\label{fig:Nube_de_puntos_con_las_diferentes_clases_del dataset_de_waymo}
\end{figure}

En la figura \ref{fig:Nube_de_puntos_con_las_diferentes_clases_del dataset_de_waymo} encontramos las diferentes clases a detectar y seguir en el dataset, teniendo en amarillo los coches, rojo los peatones, azul las señales y rosa los ciclistas.\par
Waymo ofrece un dataset muy completo si solo se desea trabajar con tareas de percepción, compitiendo cara a cara con los datasets más importantes como son KITTI, nuScenes y Argoverse, teniendo además uno de los sistemas de evaluación para tareas de percepción orientadas a vehículos autónomos, más grandes y con más anotaciones que se pueden encontrar de forma Open-Source.

\newpage

\subsection{nuScenes}
\label{sec:nuscenes}

NuScenes Dataset \cite{nuscenes_dataset} se presenta como una mejora a al dataset KITTI lanzado en 2012. Esta mejora no solo radica en la calidad de los datos sino en el número de diferentes situaciones disponibles, de la misma manera que Waymo, ofreciendo situaciones nocturnas y días lluviosos.\par
En comparación con otros datasets, se incluye un set más completo de sensores como son:
\begin{itemize}
	\item Sistema de 6 cámaras 360º con una resolución de 1600 x 900.
	\item \acs{lidar} de 32 haces con una frecuencia de 20 Hz, capaz de generar hasta 1,4 millones de puntos por segundo.
	\item Sistema de 5 \acs{radar} con una distancia máxima de 250 metros y una frecuencia de 13 Hz.
	\item Sistema de localización compuesto por \acs{gps}, \acs{imu}, \acs{ahrs} y un sistema de posicionamiento \acs{rtk}.
\end{itemize}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.6\textwidth]{figures/4_sistemas_de_percepcion_con_lidar_basados_en_deep_learning/nuscenes_car.png}
	\caption{Vehículo utilizado en el dataset de nuScenes.}
	\label{fig:Vehiculo_utilizado_en_el_dataset_de_nuscenes}
\end{figure}

NuScenes tiene 23 clases diferentes a detectar, entre ellas se encuentran como en el resto de datasets: coches, ciclistas, peatones, etc. Pero en este se encuentran además animales, diferenciación por el tipo de los peatones, como serían policías o personas en silla de ruedas, además de barreras entre otras. Esto permite agrupar las clases para detectar unicamente las más simples o servir como un sistema de evaluación con toda la información necesaria para la implementación en un sistema \acs{ads}.\par
El sistema de evaluación de nuScenes permite evaluación de múltiples sistemas a diferentes niveles. Las diferentes tareas a evaluar son:
\begin{itemize}
	\item Detección de objetos 3D (10 tipos de objetos diferentes) utilizando cámara, \acs{lidar}, \acs{radar} y la información de los mapas.
	\item Seguimiento de objetos 3D (7 tipos de objetos diferentes) utilizando cámara, \acs{lidar}, \acs{radar} y la información de los mapas.
	\item Predicción del movimiento y de la posición de los objetos.
	\item Segmentación de la nube de puntos del \acs{lidar} a nivel de punto.
\end{itemize}
El dataset completo de nuScenes ocupa medio terabyte, esto es debido a la cantidad de información que este tiene, la cual es encuentra dividida en: mapas, información de los sensores en los frames que se tienen anotaciones, información de los sensores sin anotaciones y una carpeta v1.0-trainval que contiene todos los archivos json que relacionan todo el dataset además del ground truth \ref{for:Estructura_del_dataset_nuscenes}.

\begin{figure}[H]
	\begin{forest}
	for tree={
    	font=\ttfamily,
        grow'=0,
        child anchor=west,
        parent anchor=south,
        anchor=west,
        calign=first,
        inner xsep=7pt,
        edge path={
        	\noexpand\path [draw, \forestoption{edge}]
          	(!u.south west) +(7.5pt,0) |- (.child anchor) pic {folder} \forestoption{edge label};
        },
        % style for your file node 
        file/.style={edge path={\noexpand\path [draw, \forestoption{edge}]
          	(!u.south west) +(7.5pt,0) |- (.child anchor) \forestoption{edge label};},
        	inner xsep=2pt,font=\small\ttfamily
                     },
        before typesetting nodes={
        	if n=1
            {insert before={[,phantom]}}
            {}
        },
        fit=band,
        before computing xy={l=15pt},
	}  
    [nuScenes Dataset
        [v1.0-trainval
          [maps
          ]
          [samples
          	[CAM\_BACK
          	]
          	[CAM\_BACK\_LEFT
          	]
          	[CAM\_BACK\_RIGHT
          	]
          	[CAM\_FRONT
          	]
          	[CAM\_FRONT\_LEFT
          	]
          	[CAM\_FRONT\_RIGHT
          	]
          	[LIDAR\_TOP
          	]
          	[RADAR\_BACK\_LEFT
          	]
          	[RADAR\_BACK\_RIGHT
          	]
          	[RADAR\_FRONT
          	]
          	[RADAR\_FRONT\_LEFT
          	]
          	[RADAR\_FRONT\_RIGHT
          	]
          ]
          [sweeps
          	[CAM\_BACK
          	]
          	[CAM\_BACK\_LEFT
          	]
          	[CAM\_BACK\_RIGHT
          	]
          	[CAM\_FRONT
          	]
          	[CAM\_FRONT\_LEFT
          	]
          	[CAM\_FRONT\_RIGHT
          	]
          	[LIDAR\_TOP
          	]
          	[RADAR\_BACK\_LEFT
          	]
          	[RADAR\_BACK\_RIGHT
          	]
          	[RADAR\_FRONT
          	]
          	[RADAR\_FRONT\_LEFT
          	]
          	[RADAR\_FRONT\_RIGHT
          	]
          ]
          [v1.0-trainval
          ]
        ]
    ]
	\end{forest}
\caption{Estructura del dataset nuScenes.}
\label{for:Estructura_del_dataset_nuscenes}
\end{figure}

Dicha estructura es para el uso de todos los sensores del dataset y los mapas, pero en el caso de que se requiera unicamente de las cámaras, nuScenes ofrece nuImages, este es el dataset de nuScenes reducido, que además cambia la estructura interna de este.

\begin{figure}[H]
	\begin{minipage}{0.48\textwidth}
		\centering
		\includegraphics[width=1\linewidth]{figures/4_sistemas_de_percepcion_con_lidar_basados_en_deep_learning/nuscenes_schema.png}
		\caption{Esquema de nuScenes.}
		\label{fig:Esquema_de_nuscenes}
	\end{minipage}\hfill
	\begin{minipage}{0.48\textwidth}
		\centering
		\includegraphics[width=1\linewidth]{figures/4_sistemas_de_percepcion_con_lidar_basados_en_deep_learning/nuimages_schema.png}
		\caption{Esquema de nuImages}
		\label{fig:Esquema_de_nuimages}
	\end{minipage}
\end{figure}

Las estructuras de estas versiones del dataset, no son sencillas de trabajar con los datos directamente, por lo que nuScenes ha desarrollado un devkit con el que sea más sencillo de trabajar con ambas estructuras tal y como se explicará en el siguiente apartado.

\subsubsection{NuScenes devkit}
\label{sec:nuscenes_devkit}

Promover el uso de un dataset es importante en la medida que sino se tienen características nuevas en este o es difícil de utilizar, este no destacará y desaparecerá entre la cantidad de datasets que se encuentran hoy en día. Para ello, nuScenes desarrolla y lanza de forma Open-Source nuScenes devkit.\par
NuScenes devkit, ofrece un uso muy simplificado del dataset de nuScenes y nuImages. Ambos datasets incorporan a partir de json una estructura similar a la de una base de datos relacional en la que a partir de claves primarias, externas e índices se consigue tener un conjunto de datos normalizado, para así minimizar al máximo la redundancia de datos.\par
Al ofrecer una gran cantidad de datos no solo de los sensores sino de la escenas, configuraciones de calibración, visibilidad, categorías, etc. Se ha estructurado en diferentes archivos json para no tener que repetir datos por cada imagen o nube de puntos. El devkit ha sido programado para su uso en Python 3 y los tutoriales para aprender a utilizar dicho devkit se encuentran en Jupyter Notebooks, aunque también se recomienda su uso en Google Colab.\par
Para el aprendizaje del devkit de nuScenes se descarga la versión mini del dataset, el cual tiene la misma estructura que el dataset completo. Junto con Jupyter se estudia todo el dataset siguiendo los diferentes tutoriales que ofrece. Tras trabajar con la herramienta y acostumbrase al uso de los tokens que relacionan los diferentes archivos json, es muy sencillo obtener toda la obtener los datos requeridos.\par
A partir del devkit es muy sencillo realizar tareas como filtrado de clases, agrupaciones, transformaciones mundo a cámara, seguimiento de objetos individualizado, etc. De esta manera se ahorra mucho tiempo en la construcción de complejas funciones ya  que se encuentran insertadas en el kit de desarrollo, además de que permite ver de forma analítica el comportamiento del dataset.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.6\textwidth]{figures/4_sistemas_de_percepcion_con_lidar_basados_en_deep_learning/lidar_segmentation.png}
	\caption{Transformación mundo a imagen de la nube de puntos segmentada.}
	\label{fig:Transformacion_mundo_a_imagen_de_la_nube_de_puntos_segmentada}
\end{figure}

Con código tan simple como el siguiente \ref{cod:Obtencion_de_una_nube_de_puntos_segmentada_sobre_una_imagen_utilizando_nuscenes_devkit} es muy fácil obtener la imagen de una cámara con la nube de puntos segmentada tras realizar una transformación de mundo a cámara tal y como se ve en la figura \ref{fig:Transformacion_mundo_a_imagen_de_la_nube_de_puntos_segmentada}. Dichas transformaciones como se verá en el capítulo \ref{cha:ad_devkit} no es algo inmediato, por lo es necesario cierto conocimiento del funcionamiento de las cámaras y de transformaciones geométricas.

\begin{codefloat}
\begin{lstlisting}[language=Python]
from nuscenes import NuScenes

nusc = NuScenes(version='v1.0-mini',
                dataroot='/home/javier/nuScenes-dataset-mini-v1.0',
                verbose=True)
my_sample = nusc.sample[87]

nusc.render_pointcloud_in_image(my_sample['token'],
                                pointsensor_channel='LIDAR_TOP',
                                camera_channel='CAM_BACK',
                                render_intensity=False,
                                show_lidarseg=True,
                                filter_lidarseg_labels=[22, 23],
                                show_lidarseg_legend=True)

\end{lstlisting}
\caption{Obtención de una nube de puntos segmentada sobre una imagen utilizando nuScenes devkit}
\label{cod:Obtencion_de_una_nube_de_puntos_segmentada_sobre_una_imagen_utilizando_nuscenes_devkit}
\end{codefloat}

\subsection{Comparativa entre los diferentes datasets}
\label{sec:comparativa_entre_los_diferentes_datasets}

Tras el estudio y análisis de los datasets de KITTI, Waymo y nuScenes se obtiene un conocimiento de las arquitecturas hardware utilizadas en el estado de arte del campo de los datasets de conducción autónoma, además de los sistemas de evaluación utilizados.\par
Mientras que KITTI fue el primero en desarrollo en un dataset para conducción autónoma, hasta hace muy poco estaba casi estandarizado en la comparativa de modelos  aplicables a conducción autónoma. Waymo y nuScenes aparecieron años más tarde con una apuesta clara en el uso de más sensores, además del estudio de los 360º del entorno y no solo de la parte frontal del vehículo como realiza KITTI.\par
En la parte del hardware, se ha visto que el acercamiento por parte de KITTI de tener 4 cámaras es un enfoque equivocado para un coche real, ya que se necesita tener un rango de visión mayor en el sistema de cámaras. Por parte del uso del \acs{lidar}, se encuentra con que KITTI solo se usa la parte frontal de la nube de puntos, mientras que Waymo y nuScenes utilizan toda la nube de puntos para generar las detecciones. Entre estos dos datasets encontramos diferencias en la cantidad de \acs{lidar} utilizados, ya que mientras nuScenes utiliza solo uno, Waymo utiliza cinco, lo cual obtiene mucha más información del entorno y una nube de puntos más densa, pero aumenta en mayor medida el precio de un prototipo con estas características. El \acs{radar} es un sensor desaparecido tanto en KITTI como en Waymo pero que se encuentra en nuScenes ofreciendo una nube de puntos de 360º que es capaz de inferir la velocidad de los objetos pero con una menor cantidad de puntos que una proveniente del \acs{lidar}.

\begin{table}[H]
	\begin{center}
		\begin{tabular}{|c|c|c|c|c|c|c|c|c|} 
		\hline
		Dataset & Año & Situaciones & Horas & Cámaras & LiDAR & RADAR & Clases & Devkit\\ 
		\hline\hline
		KITTI & 2012 & 22 & 1,5 & 4 & 1 & 0 & 8 & Sí \\ 
		\hline
		Waymo & 2019 & 1000 & 5,5 & 5 & 5 & 0 & 4 & No \\
		\hline
		nuScenes & 2019 & 1150 & 5,5 & 6 & 1 & 5 & 23 & Sí \\
		\hline
		\end{tabular}
		\caption{Comparativa entre los principales datasets.}
		\label{tab:Comparativa_entre_los_principales_datasets}
	\end{center}
\end{table}

En relación a la cantidad de situaciones y de datos Waymo y nuScenes se encuentran en un estado similar con una gran cantidad de datos del entorno, mientras que KITTI se queda más atrás debido a la longevidad del dataset.\par
Unicamente KITTI y nuScenes cuentan con un devkit, este es utilizado para uso más simplificado del dataset, aunque en el caso de nuScenes es casi requerido su uso. El problema de KITTI en este aspecto es el uso de Matlab para el uso del devkit ya que la mayoría de la comunidad investigadora no utiliza este lenguaje para la creación de los modelos de detección, aunque se pueden encontrar de forma no oficial, repositorios Open-Source que ofrecen variantes del devkit de KITTI en otros lenguajes.\par
En conclusión, KITTI ha sido una gran base para la generación de la siguiente generación de datasets, aunque para sistemas de percepción algo más complejos se puede quedar corto, lo cual se ha ido mejorando con el tiempo y por esto mismo se está desarrollando KITII-360. Por otra parte, Waymo y nuScenes ofrecen un dataset más completo, aunque nuScenes ofrece información de un sensor más y contiene la información de los mapas.

\section{Estado del arte en detección utilizando LiDAR}
\label{sec:estado_del_arte_en_deteccion_utilizando_lidar}

Las técnicas basadas en \acs{dl} que hacen uso de \acs{cnn} llevan unos años siendo \acs{sota} en el campo de la detección utilizando unicamente \acs{lidar}. En este apartado se van a estudiar los principales modelos de detección en este campo que hacen uso de estas técnicas.

\subsection{PointPillars}
\label{sec:pointpillars}

\cite{cbgs}

\subsection{SECOND}
\label{sec:second}



\subsection{PointRCNN}
\label{sec:pointrcnn}



\subsection{PV-RCNN}
\label{sec:pv_rcnn}



\subsection{CBGS}
\label{sec:cbgs}



\section{OpenPCDet}
\label{sec:openpcdet}


