\chapter{Sistemas de percepción con LiDAR basados en Deep Learning}
\label{cha:sistemas_de_percepcion_con_lidar_basados_en_deep_learning}

\begin{FraseCelebre}
  \begin{Frase}
    Si no conozco una cosa, la investigaré.
  \end{Frase}
  \begin{Fuente}
    Louis Pasteur
  \end{Fuente}
\end{FraseCelebre}

\noindent
Los sistemas de percepción pertenecientes al estado del arte o \ac{sota} se encuentran basados en \acs{dl}, esto no es diferente en los sistemas de percepción basados en \acs{lidar}, por lo que en este capítulo se presentan los datasets disponibles para el entrenamiento y evaluación de los modelos, las diferentes arquitecturas \acs{sota} para detección con \acs{lidar} y la herramienta utilizada para la evaluación, entrenamiento y pruebas realizadas sobre los modelos.

\section{Principales datasets}
\label{sec:principales_datasets}

Para el desarrollo de un modelo de percepción basado en \acs{dl}, es siempre necesario un conjunto de datos anotados o dataset sobre el que un modelo pueda aprender a partir de estos. En los últimos años muchas compañías han lanzado datasets para poder entrenar y validar sus modelos, además de que se muchos de estos datasets han sido publicados Open-Source para fomentar el desarrollo de nuevas técnicas.\par
Entre los datasets para conducción autónoma encontramos: A2D2 Dataset \cite{a2d2_dataset}, Argoverse Dataset \cite{argoverse_dataset}, CityScapes Dataset \cite{cityscapes_dataset}, KITTI Vision Benchmark Suite \cite{kitti_dataset}, Level 5 Open Data \cite{level_5_dataset}, nuScenes Dataset \cite{nuscenes_dataset} o Waymo Open Dataset \cite{waymo_dataset} entre otros.\par
En este apartado se van a presentar tres de los datasets más importante en la industria del automóvil, como son KITTI, nuScenes y Waymo dataset.

\subsection{KITTI}
\label{sec:kitti}

La suite de evaluación de KITTI \cite{kitti_dataset} es un sistema de evaluación para vehículos autónomos, donde se ha desarrollado una plataforma de referencia para tareas de visión estereoscópica, flujo óptico, odometría visual/\acs{slam} y detección de objetos 3D.\par
Este dataset fue presentado en 2012 en conjunto por el \acs{kit} y el \acs{ttic}, iniciando lo que años después despertaría un interés en la creación de datasets Open-Source para sistemas \acs{adas}/\acs{ads}. Al ser el primer dataset con reconocimiento internacional, fijó las bases de lo que sería el futuro de los sistemas de evaluación, además que ha sido desde su salida uno de los sistemas de evaluación más estandarizados en el desarrollo de técnicas de conducción autónoma, como se puede ver en sus más de 7.000 citas.

\begin{figure}[H]
	\begin{forest}
	for tree={
    	font=\ttfamily,
        grow'=0,
        child anchor=west,
        parent anchor=south,
        anchor=west,
        calign=first,
        inner xsep=7pt,
        edge path={
        	\noexpand\path [draw, \forestoption{edge}]
          	(!u.south west) +(7.5pt,0) |- (.child anchor) pic {folder} \forestoption{edge label};
        },
        % style for your file node 
        file/.style={edge path={\noexpand\path [draw, \forestoption{edge}]
          	(!u.south west) +(7.5pt,0) |- (.child anchor) \forestoption{edge label};},
        	inner xsep=2pt,font=\small\ttfamily
                     },
        before typesetting nodes={
        	if n=1
            {insert before={[,phantom]}}
            {}
        },
        fit=band,
        before computing xy={l=15pt},
	}  
    [KITTI Dataset
      [2011\_09\_26
        [2011\_09\_26\_drive\_0001\_sync
          [image\_00
            [data
              [0000000000.txt,file
              ]
              [0000000???.txt,file
              ]
            ]
            [timestamps.txt,file
            ]
          ]
          [image\_01
            [...,file
    	  	]
          ]
          [image\_02
            [...,file
    	  	]
          ]
          [image\_03
            [...,file
    	  	]
          ]
          [oxts
            [data
              [0000000000.txt,file
              ]
              [0000000???.txt,file
              ]
            ]
            [dataformat.txt,file
            ]
            [timestamps.txt,file
            ]
          ]
          [velodyne\_points
            [data
              [0000000000.bin,file
              ]
              [0000000???.bin,file
              ]
            ]
            [timestamps.txt,file
            ]
            [timestamps\_end.txt,file
            ]
            [timestamps\_start.txt,file
            ]
          ]
    	]
    	[2011\_09\_26\_drive\_0???\_sync
    	  [...,file
    	  ]
    	]
    	[...,file
    	]
      ]
      [20??\_??\_??
      	[...,file
      	]
      ]
      [...,file
      ]
    ]
	\end{forest}
\caption{Estructura del dataset de KITTI.}
\label{for:Estructura_del_dataset_kitti}
\end{figure}

\newpage

El sistema de percepción del vehículo utilizado para la grabación de los datos se encuentra compuesto de:
\begin{itemize}
	\item 2 sistemas de cámaras estéreo de una resolución de 1240 x 376 píxeles.
	\item 1 \acs{lidar} Velodyne HDL-64E capaz de generar más de un millón de puntos por segundo gracias a sus 64 haces láser.
	\item 1 sistema de localización \acs{sota} compuesto por \acs{gps}, \acs{gnss}, \acs{imu} y corrección de señales \acs{rtk}.
\end{itemize}

\begin{figure}[H]
	\begin{minipage}{0.48\textwidth}
		\centering
		\includegraphics[width=1\linewidth]{figures/4_sistemas_de_percepcion_con_lidar_basados_en_deep_learning/kittis_car.png}
		\caption{Vehículo utilizado en KITTI.}
		\label{fig:Vehiculo_utilizado_en_kitti}
	\end{minipage}\hfill
	\begin{minipage}{0.48\textwidth}
		\centering
		\includegraphics[width=1\linewidth]{figures/4_sistemas_de_percepcion_con_lidar_basados_en_deep_learning/kittis_car_measures.png}
		\caption{Medidas del vehículo utilizado en KITTI}
		\label{fig:Medidas_del_vehiculo_utilizado_en_kiti}
	\end{minipage}
\end{figure}

La estructura del dataset es tal y como se muestra en la figura \ref{for:Estructura_del_dataset_kitti}. Dentro de este los datos se dividen según el día de grabación y la escena, y dentro de cada escena se tienen las imágenes de la cámara estéreo monocromática (image\_00, image\_01) y RGB (image\_02, image\_03), información de todo el sistema de locación del vehículo (oxts) y las nubes de puntos provenientes del \acs{lidar} (velodyne\_points).\par
En 2020 tras el éxito que ha sido este dataset se anuncia y se abre al uso KITTI-360 \cite{kitti_360_dataset} la evolución del dataset KITTI, el cual trae mejoras añadiendo dos cámaras de ojo de pez para una obtención de visión 2D en todos los ángulos del vehículo, y una unidad láser adicional SICK. Además se incluyen en el dataset bounding boxes 3D de todo el entorno, anotaciones de instancia a partir de la información del \acs{lidar} y anotaciones de confianza sobre el vídeo tomado por las cámaras, entre otras.

\subsubsection{Análisis de la estructura del GT y las PCLs de KITTI}
\label{sec:analisis_de_la_estructura_del_gt_y_las_pcls_de_kitti}

Al estar basado este trabajo en la detección utilizando \acs{lidar}, se estudia la forma en la que se guardan los datos de las nubes de puntos junto con la representación del ground truth, para ello se analiza tanto la carpeta de velodyne\_points como el archivo tracklets.xml.\par
Para el trabajo con bounding boxes 3D, es posible utilizar unicamente el kit de detecciones 3D que contiene un conjunto de txt con la información de los objetos por cada barrido. La información de estos archivos contiene la información de: tipo, truncado, ocluido, bounding box 2D, dimensiones y localización, toda esta información se encuentra de la misma manera en el archivo tracklets.xml, pero la información de rotación se encuentra divida en un ángulo alpha y otro rotation\_y, en vez de rotación por cada uno de los tres ejes, de estos se estudiará más en profundidad en el capítulo \ref{cha:ad_devkit}.\par
Se utiliza el archivo tracklets.xml ya define el ground truth de la escena pero no por cada barrido del \acs{lidar}, sino que se guarda por cada escenario, lo que se traduce en un estructura se puede utilizar tanto para detección como para seguimiento de los objetos, todo ello como un archivo con formato \acs{xml}.\par
Se ha decido crear un programa que lea la nube de puntos indicada y que marque en un entorno 3D donde se encuentran los objetos de la escena. Esto podría ser realizado mediante el devkit que KITTI ofrece, pero este se encuentra unicamente de forma oficial en Matlab, por lo que como los modelos de \acs{dl} a utilizar se encuentran utilizando Pytorch, se deberían de analizar los datos utilizando Python.\par
La figura \ref{for:Estructura_del_archivo_tracket_labels_xml} muestra la estructura de los principales atributos del archivo tracklet\_labels.xml analizado, en este encontramos el número de los diferentes objetos del escenario, tras esto se analiza cada objeto a través de los diferentes barridos, mostrando primero las características de los objetos que se mantienen en el tiempo, como las dimensiones o el tipo de objeto y tras esto se guarda la información de posición, rotación, etc.

\begin{figure}[H]
	\begin{forest}
	for tree={
    	font=\ttfamily,
        grow'=0,
        child anchor=west,
        parent anchor=south,
        anchor=west,
        calign=first,
        inner xsep=7pt,
        edge path={
        	\noexpand\path [draw, \forestoption{edge}]
          	(!u.south west) +(7.5pt,0) |- (.child anchor) pic {folder} \forestoption{edge label};
        },
        % style for your file node 
        file/.style={edge path={\noexpand\path [draw, \forestoption{edge}]
          	(!u.south west) +(7.5pt,0) |- (.child anchor) \forestoption{edge label};},
        	inner xsep=2pt,font=\small\ttfamily
                     },
        before typesetting nodes={
        	if n=1
            {insert before={[,phantom]}}
            {}
        },
        fit=band,
        before computing xy={l=15pt},
	}  
    [tracklet\_labels.xml
      [tracklets,file
        [count,file
        ]
        [item,file
          [objectType,file
          ]
          [h,file
          ]
          [w,file
          ]
          [l,file
          ]
          [first\_frame,file
          ]
          [poses,file
            [count,file
            ]
            [item,file
              [tx,file
              ]
              [ty,file
              ]
              [tz,file
              ]
              [rx,file
              ]
              [ry,file
              ]
              [rz,file
              ]
              [state,file
              ]
              [occlusion,file
              ]
              [truncation,file
              ]
            ]
            [...,file]
          ]
        ]
        [...,file
        ]
      ]
    ]
	\end{forest}
\caption{Estructura del archivo tracklet\_labels.xml.}
\label{for:Estructura_del_archivo_tracket_labels_xml}
\end{figure}

Tras dicho análisis, utilizando la librería MayaVi para la visualización del entorno 3D, NumPy para las transformaciones en el espacio y Beautiful Soup para la lectura del \acs{xml}, se visualizan múltiples escenas del dataset.\par
Cabe destacar que como KITTI está realizado para que la parte de percepción se trabaje principalmente con la cámaras frontales del vehículo, aún teniendo información por los laterales y la parte trasera gracias al \acs{lidar}, esta no se tiene en cuenta en el ground truth, razón por la que en la imagen \ref{fig:Visualizacion_de_una_nube_de_puntos_de_kitti_junto_con_su_ground_truth} los vehículos de la parte trasera no se muestra.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.55\textwidth]{figures/4_sistemas_de_percepcion_con_lidar_basados_en_deep_learning/display_kitti_example.png}
	\caption{Visualización de una nube de puntos de KITTI junto con su ground truth.}	\label{fig:Visualizacion_de_una_nube_de_puntos_de_kitti_junto_con_su_ground_truth}
\end{figure}

Todo el código utilizado para la visualización de los objetos junto con unos escenarios de ejemplo son accesibles en: \url{https://github.com/Javier-DlaP/Display_kitti_pcl_annotations} 

\subsection{Waymo}
\label{sec:waymo}

Waymo Open Dataset \cite{waymo_dataset} es el dataset liberado de forma Open-Source por parte de Waymo y Google para la aceleración del desarrollo de tecnologías de conducción autónoma.\par
La propuesta de este dataset es la oferta de un gran número de anotaciones de alta calidad tanto 2D como 3D,  que además contienen información de seguimiento. Se han utilizado múltiples ciudades para sus escenario grabados, como son: San Francisco, Mountain View, Los Angeles, Detroit, Seattle y Phoenix. Además se trabaja con multitud de entornos y condiciones ambientales como son: construcciones, atardeceres, noches, días lluviosos, etc.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.6\textwidth]{figures/4_sistemas_de_percepcion_con_lidar_basados_en_deep_learning/waymos_car.png}
	\caption{Vehículo utilizado en Waymo.}
	\label{fig:Vehículo_utilizado_en_waymo}
\end{figure}

\newpage

Actualmente el dataset ofrece 1.950 escenas \cite{waymo_web_page} que fueron aumentadas de las 1150 escenas en la salida de la primera revisión del paper, de 20 segundos cada una,
con información recogida a 10 Hz proveniente de los sensores, lo que implican
390.000 frames. Toda esta información es recabada de:
\begin{itemize}
	\item 1 \acs{lidar} de medio alcance.
	\item 4 \acs{lidar} de corto alcance.
	\item 5 cámaras alrededor del vehículo.
\end{itemize}
Como se ve en la figura \ref{fig:Vehículo_utilizado_en_waymo}, el acercamiento de la compañía entorno a la construcción del vehículo no utiliza \acs{radar}, depende unicamente de cámaras y \acs{lidar} para el apartado de percepción. Por lo que no se puede trabajar con \acs{radar} en el dataset, además de que no se tiene información de ningún sistema de localización, ya que dicho dataset se encuentra especializado en tareas de percepción y seguimiento de los objetos de la escena.\par
Es importante tener en cuenta que en este dataset, la nube de puntos procedente del \acs{lidar} no utiliza un sistema de coordenadas cartesiano, sino un sistema de coordenadas esférico, donde las coordenadas (x, y, z) son reemplazadas por (distancia, azimuth, inclinación).

\begin{center}
$distancia = \sqrt{x²+y²+z²}$\\[10pt]
$azimuth = \atantwo(y, x)$\\[10pt]
$inclinación = \atantwo(z, \sqrt{x²+y²})$
\end{center}

Mientras que en otros datasets se tienen multitud de clases, muchas de ellas indistinguibles unas de otras, Waymo utiliza unicamente 4 tipos de objetos diferentes. En las 11,8 millones de bounding boxes 2D se encuentran vehículos, peatones y ciclistas, mientras que en las 12,6 millones de bounding boxes 3D se añaden además la detección y seguimiento de señales.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.6\textwidth]{figures/4_sistemas_de_percepcion_con_lidar_basados_en_deep_learning/classes_waymo.png}
	\caption{Nube de puntos con las diferentes clases del dataset de Waymo.}
	\label{fig:Nube_de_puntos_con_las_diferentes_clases_del dataset_de_waymo}
\end{figure}

En la figura \ref{fig:Nube_de_puntos_con_las_diferentes_clases_del dataset_de_waymo} encontramos las diferentes clases a detectar y seguir en el dataset, teniendo en amarillo los coches, rojo los peatones, azul las señales y rosa los ciclistas.\par
Waymo ofrece un dataset muy completo si solo se desea trabajar con tareas de percepción, compitiendo cara a cara con los datasets más importantes como son KITTI, nuScenes y Argoverse, teniendo además uno de los sistemas de evaluación para tareas de percepción orientadas a vehículos autónomos, más grandes y con más anotaciones que se pueden encontrar de forma Open-Source.

\newpage

\subsection{nuScenes}
\label{sec:nuscenes}

NuScenes Dataset \cite{nuscenes_dataset} se presenta como una mejora a al dataset KITTI lanzado en 2012. Esta mejora no solo radica en la calidad de los datos sino en el número de diferentes situaciones disponibles, de la misma manera que Waymo, ofreciendo situaciones nocturnas y días lluviosos.\par
En comparación con otros datasets, se incluye un set más completo de sensores como son:
\begin{itemize}
	\item Sistema de 6 cámaras 360º con una resolución de 1600 x 900.
	\item \acs{lidar} de 32 haces con una frecuencia de 20 Hz, capaz de generar hasta 1,4 millones de puntos por segundo.
	\item Sistema de 5 \acs{radar} con una distancia máxima de 250 metros y una frecuencia de 13 Hz.
	\item Sistema de localización compuesto por \acs{gps}, \acs{imu}, \acs{ahrs} y un sistema de posicionamiento \acs{rtk}.
\end{itemize}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.6\textwidth]{figures/4_sistemas_de_percepcion_con_lidar_basados_en_deep_learning/nuscenes_car.png}
	\caption{Vehículo utilizado en el dataset de nuScenes.}
	\label{fig:Vehiculo_utilizado_en_el_dataset_de_nuscenes}
\end{figure}

NuScenes tiene 23 clases diferentes a detectar, entre ellas se encuentran como en el resto de datasets: coches, ciclistas, peatones, etc. Pero en este se encuentran además animales, diferenciación por el tipo de los peatones, como serían policías o personas en silla de ruedas, además de barreras entre otras. Esto permite agrupar las clases para detectar unicamente las más simples o servir como un sistema de evaluación con toda la información necesaria para la implementación en un sistema \acs{ads}.\par
El sistema de evaluación de nuScenes permite evaluación de múltiples sistemas a diferentes niveles. Las diferentes tareas a evaluar son:
\begin{itemize}
	\item Detección de objetos 3D (10 tipos de objetos diferentes) utilizando cámara, \acs{lidar}, \acs{radar} y la información de los mapas.
	\item Seguimiento de objetos 3D (7 tipos de objetos diferentes) utilizando cámara, \acs{lidar}, \acs{radar} y la información de los mapas.
	\item Predicción del movimiento y de la posición de los objetos.
	\item Segmentación de la nube de puntos del \acs{lidar} a nivel de punto.
\end{itemize}
El dataset completo de nuScenes ocupa medio terabyte, esto es debido a la cantidad de información que este tiene, la cual es encuentra dividida en: mapas, información de los sensores en los frames que se tienen anotaciones, información de los sensores sin anotaciones y una carpeta v1.0-trainval que contiene todos los archivos json que relacionan todo el dataset además del ground truth \ref{for:Estructura_del_dataset_nuscenes}.

\begin{figure}[H]
	\begin{forest}
	for tree={
    	font=\ttfamily,
        grow'=0,
        child anchor=west,
        parent anchor=south,
        anchor=west,
        calign=first,
        inner xsep=7pt,
        edge path={
        	\noexpand\path [draw, \forestoption{edge}]
          	(!u.south west) +(7.5pt,0) |- (.child anchor) pic {folder} \forestoption{edge label};
        },
        % style for your file node 
        file/.style={edge path={\noexpand\path [draw, \forestoption{edge}]
          	(!u.south west) +(7.5pt,0) |- (.child anchor) \forestoption{edge label};},
        	inner xsep=2pt,font=\small\ttfamily
                     },
        before typesetting nodes={
        	if n=1
            {insert before={[,phantom]}}
            {}
        },
        fit=band,
        before computing xy={l=15pt},
	}  
    [nuScenes Dataset
        [v1.0-trainval
          [maps
          ]
          [samples
          	[CAM\_BACK
          	]
          	[CAM\_BACK\_LEFT
          	]
          	[CAM\_BACK\_RIGHT
          	]
          	[CAM\_FRONT
          	]
          	[CAM\_FRONT\_LEFT
          	]
          	[CAM\_FRONT\_RIGHT
          	]
          	[LIDAR\_TOP
          	]
          	[RADAR\_BACK\_LEFT
          	]
          	[RADAR\_BACK\_RIGHT
          	]
          	[RADAR\_FRONT
          	]
          	[RADAR\_FRONT\_LEFT
          	]
          	[RADAR\_FRONT\_RIGHT
          	]
          ]
          [sweeps
          	[CAM\_BACK
          	]
          	[CAM\_BACK\_LEFT
          	]
          	[CAM\_BACK\_RIGHT
          	]
          	[CAM\_FRONT
          	]
          	[CAM\_FRONT\_LEFT
          	]
          	[CAM\_FRONT\_RIGHT
          	]
          	[LIDAR\_TOP
          	]
          	[RADAR\_BACK\_LEFT
          	]
          	[RADAR\_BACK\_RIGHT
          	]
          	[RADAR\_FRONT
          	]
          	[RADAR\_FRONT\_LEFT
          	]
          	[RADAR\_FRONT\_RIGHT
          	]
          ]
          [v1.0-trainval
          ]
        ]
    ]
	\end{forest}
\caption{Estructura del dataset nuScenes.}
\label{for:Estructura_del_dataset_nuscenes}
\end{figure}

Dicha estructura es para el uso de todos los sensores del dataset y los mapas, pero en el caso de que se requiera unicamente de las cámaras, nuScenes ofrece nuImages, este es el dataset de nuScenes reducido, que además cambia la estructura interna de este.

\begin{figure}[H]
	\begin{minipage}{0.48\textwidth}
		\centering
		\includegraphics[width=1\linewidth]{figures/4_sistemas_de_percepcion_con_lidar_basados_en_deep_learning/nuscenes_schema.png}
		\caption{Esquema de nuScenes.}
		\label{fig:Esquema_de_nuscenes}
	\end{minipage}\hfill
	\begin{minipage}{0.48\textwidth}
		\centering
		\includegraphics[width=1\linewidth]{figures/4_sistemas_de_percepcion_con_lidar_basados_en_deep_learning/nuimages_schema.png}
		\caption{Esquema de nuImages}
		\label{fig:Esquema_de_nuimages}
	\end{minipage}
\end{figure}

Las estructuras de estas versiones del dataset, no son sencillas de trabajar con los datos directamente, por lo que nuScenes ha desarrollado un devkit con el que sea más sencillo de trabajar con ambas estructuras tal y como se explicará en el siguiente apartado.

\subsubsection{NuScenes devkit}
\label{sec:nuscenes_devkit}

Promover el uso de un dataset es importante en la medida que sino se tienen características nuevas en este o es difícil de utilizar, este no destacará y desaparecerá entre la cantidad de datasets que se encuentran hoy en día. Para ello, nuScenes desarrolla y lanza de forma Open-Source nuScenes devkit.\par
NuScenes devkit, ofrece un uso muy simplificado del dataset de nuScenes y nuImages. Ambos datasets incorporan a partir de json una estructura similar a la de una base de datos relacional en la que a partir de claves primarias, externas e índices se consigue tener un conjunto de datos normalizado, para así minimizar al máximo la redundancia de datos.\par
Al ofrecer una gran cantidad de datos no solo de los sensores sino de la escenas, configuraciones de calibración, visibilidad, categorías, etc. Se ha estructurado en diferentes archivos json para no tener que repetir datos por cada imagen o nube de puntos. El devkit ha sido programado para su uso en Python 3 y los tutoriales para aprender a utilizar dicho devkit se encuentran en Jupyter Notebooks, aunque también se recomienda su uso en Google Colab.\par
Para el aprendizaje del devkit de nuScenes se descarga la versión mini del dataset, el cual tiene la misma estructura que el dataset completo. Junto con Jupyter se estudia todo el dataset siguiendo los diferentes tutoriales que ofrece. Tras trabajar con la herramienta y acostumbrase al uso de los tokens que relacionan los diferentes archivos json, es muy sencillo obtener toda la obtener los datos requeridos.\par
A partir del devkit es muy sencillo realizar tareas como filtrado de clases, agrupaciones, transformaciones mundo a cámara, seguimiento de objetos individualizado, etc. De esta manera se ahorra mucho tiempo en la construcción de complejas funciones ya  que se encuentran insertadas en el kit de desarrollo, además de que permite ver de forma analítica el comportamiento del dataset.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.6\textwidth]{figures/4_sistemas_de_percepcion_con_lidar_basados_en_deep_learning/lidar_segmentation.png}
	\caption{Transformación mundo a imagen de la nube de puntos segmentada en el devkit de nuScenes.}
	\label{fig:Transformacion_mundo_a_imagen_de_la_nube_de_puntos_segmentada_en_el_devkit_de_nuscenes}
\end{figure}

Con código tan simple como el siguiente \ref{cod:Obtencion_de_una_nube_de_puntos_segmentada_sobre_una_imagen_utilizando_nuscenes_devkit} es muy fácil obtener la imagen de una cámara con la nube de puntos segmentada tras realizar una transformación de mundo a cámara tal y como se ve en la figura \ref{fig:Transformacion_mundo_a_imagen_de_la_nube_de_puntos_segmentada_en_el_devkit_de_nuscenes}. Dichas transformaciones como se verá en el capítulo \ref{cha:ad_devkit} no es algo inmediato, por lo es necesario cierto conocimiento del funcionamiento de las cámaras y de transformaciones geométricas.

\begin{codefloat}
\begin{lstlisting}[language=Python]
from nuscenes import NuScenes

nusc = NuScenes(version='v1.0-mini',
                dataroot='/home/javier/nuScenes-dataset-mini-v1.0',
                verbose=True)
my_sample = nusc.sample[87]

nusc.render_pointcloud_in_image(my_sample['token'],
                                pointsensor_channel='LIDAR_TOP',
                                camera_channel='CAM_BACK',
                                render_intensity=False,
                                show_lidarseg=True,
                                filter_lidarseg_labels=[22, 23],
                                show_lidarseg_legend=True)

\end{lstlisting}
\caption{Obtención de una nube de puntos segmentada sobre una imagen utilizando nuScenes devkit}
\label{cod:Obtencion_de_una_nube_de_puntos_segmentada_sobre_una_imagen_utilizando_nuscenes_devkit}
\end{codefloat}

\subsection{Comparativa entre los diferentes datasets}
\label{sec:comparativa_entre_los_diferentes_datasets}

Tras el estudio y análisis de los datasets de KITTI, Waymo y nuScenes se obtiene un conocimiento de las arquitecturas hardware utilizadas en el estado de arte del campo de los datasets de conducción autónoma, además de los sistemas de evaluación utilizados.\par
Mientras que KITTI fue el primero en desarrollo en un dataset para conducción autónoma, hasta hace muy poco estaba casi estandarizado en la comparativa de modelos  aplicables a conducción autónoma. Waymo y nuScenes aparecieron años más tarde con una apuesta clara en el uso de más sensores, además del estudio de los 360º del entorno y no solo de la parte frontal del vehículo como realiza KITTI.\par
En la parte del hardware, se ha visto que el acercamiento por parte de KITTI de tener 4 cámaras es un enfoque equivocado para un coche real, ya que se necesita tener un rango de visión mayor en el sistema de cámaras. Por parte del uso del \acs{lidar}, se encuentra con que KITTI solo se usa la parte frontal de la nube de puntos, mientras que Waymo y nuScenes utilizan toda la nube de puntos para generar las detecciones. Entre estos dos datasets encontramos diferencias en la cantidad de \acs{lidar} utilizados, ya que mientras nuScenes utiliza solo uno, Waymo utiliza cinco, lo cual obtiene mucha más información del entorno y una nube de puntos más densa, pero aumenta en mayor medida el precio de un prototipo con estas características. El \acs{radar} es un sensor desaparecido tanto en KITTI como en Waymo pero que se encuentra en nuScenes ofreciendo una nube de puntos de 360º que es capaz de inferir la velocidad de los objetos pero con una menor cantidad de puntos que una proveniente del \acs{lidar}.

\begin{table}[H]
	\begin{center}
		\begin{tabular}{|c|c|c|c|c|c|c|c|c|} 
		\hline
		Dataset & Año & Situaciones & Horas & Cámaras & LiDAR & RADAR & Clases & Devkit\\ 
		\hline\hline
		KITTI & 2012 & 22 & 1,5 & 4 & 1 & 0 & 8 & Sí \\ 
		\hline
		Waymo & 2019 & 1000 & 5,5 & 5 & 5 & 0 & 4 & No \\
		\hline
		nuScenes & 2019 & 1150 & 5,5 & 6 & 1 & 5 & 23 & Sí \\
		\hline
		\end{tabular}
		\caption{Comparativa entre los principales datasets.}
		\label{tab:Comparativa_entre_los_principales_datasets}
	\end{center}
\end{table}

En relación a la cantidad de situaciones y de datos Waymo y nuScenes se encuentran en un estado similar con una gran cantidad de datos del entorno, mientras que KITTI se queda más atrás debido a la longevidad del dataset.\par
Unicamente KITTI y nuScenes cuentan con un devkit, este es utilizado para uso más simplificado del dataset, aunque en el caso de nuScenes es casi requerido su uso. El problema de KITTI en este aspecto es el uso de Matlab para el uso del devkit ya que la mayoría de la comunidad investigadora no utiliza este lenguaje para la creación de los modelos de detección, aunque se pueden encontrar de forma no oficial, repositorios Open-Source que ofrecen variantes del devkit de KITTI en otros lenguajes.\par
En conclusión, KITTI ha sido una gran base para la generación de la siguiente generación de datasets, aunque para sistemas de percepción algo más complejos se puede quedar corto, lo cual se ha ido mejorando con el tiempo y por esto mismo se está desarrollando KITII-360. Por otra parte, Waymo y nuScenes ofrecen un dataset más completo, aunque nuScenes ofrece información de un sensor más y contiene la información de los mapas. Debido a esto se ha decidido estudiar más en profundidad y trabajar con los dataset de KITTI y nuScenes.

\section{Estado del arte en detección utilizando LiDAR}
\label{sec:estado_del_arte_en_deteccion_utilizando_lidar}

Las técnicas basadas en \acs{dl} que hacen uso de \acs{cnn} llevan unos años siendo \acs{sota} en el campo de la detección utilizando unicamente \acs{lidar}. En este apartado se van a estudiar los principales modelos de detección en este campo que hacen uso de estas técnicas.

\subsection{SECOND}
\label{sec:second}

SECOND (Sparsely Embedded CONvolutional Detector) \cite{second} se publica en 2018 para superar los modelos de detección 3D, utilizando unicamente \acs{lidar}. Para ello propone una arquitectura basada en tres componentes principales: extractor de características a nivel de vóxel, \ac{cnn} dispersa y \ac{rpn}. Precedido todo ello de una fase de preprocesamiento de la nube de puntos y con una obtención a posteriori de las salidas del modelo.

\begin{figure}[H]
	\centering
	\includegraphics[width=1\textwidth]{figures/4_sistemas_de_percepcion_con_lidar_basados_en_deep_learning/second.png}
	\caption{Arquitectura propuesta del detector SECOND.}
	\label{fig:Arquitectura_propuesta_del_detector_second}
\end{figure}

Las diferentes fases del modelo que son utilizadas son las siguientes:
\begin{enumerate}
\item Preprocesamiento de la nube de puntos\par
Se comienza con la voxelización de la nube de puntos haciendo uso de una tabla hash, para su rápido acceso en memoria. Al tener que definir un tamaño de vóxel fijo en función de lo que se desee detectar, se ajusta el tamaño de la \acs{roi}, ya que como se trabajar con KITTI solo se utiliza la parte frontal del \acs{lidar}. Para la detección de coches y otros vehículos se usa [-3, 1] x [-40, 40] x [0, 70.4] m y para el modelo más pequeño se utiliza [-3, 1] x [-32, 32] x [0, 52.8] m para no tener que computar tantos vóxeles como el modelo completo. Todas las versiones del modelo utilizan un tamaño de vóxel fijo de [0.4, 0.2, 0.2] m y por cada uno de ellos se guarda un máximo de 35 puntos.
\item Extractor de características a nivel de vóxel\par
Se utiliza una capa \ac{vfe} tal como se presenta en el detector VoxelNet \cite{voxelnet} para extraer las características. Para ello se utiliza una \ac{fcn} compuesta de capas \acs{fc}, una capa que aplica batch nomalization y una \ac{relu} de salida para extraer las características a nivel de punto y se aplica max pooling para la obtención de las características a nivel de vóxel.
\item Extractor convolucional disperso\par
El uso de Sparse Convolutional Networks  ofrece una mejora en rendimiento de al no computar una salida si no hay una entrada dada. Al trabajar con una nube de puntos voxelizada se consigue un mejora en el rendimiento al no utilizar aquellos vóxeles que no contienen ningún punto. Para poder aplicar este tipo de redes es necesario hallar que indices del kernel van a ser utilizados, y para ello se necesita un algoritmo que contenga las reglas para indicar que parte del kernel es utilizado.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.6\textwidth]{figures/4_sistemas_de_percepcion_con_lidar_basados_en_deep_learning/sparse_convolutional_extractor.png}
	\caption{Algoritmo de convolución disperso propuesto en SECOND.}
	\label{fig:Algoritmo_de_convolucion_disperso_propuesto_en_second}
\end{figure}

Para ello se construye una tabla de matrices de reglas para guardar los indices utilizados. Con esto en cuenta, hace falta el algoritmo que genere las reglas, SparseConvNet \cite{sparseconvnet} es el modelo original que ofrece la implementación del Submanifold Convolution, una técnica dentro del campo de las Sparse Convolutional Networks que realiza la generación de reglas en CPU. En este paper se implementa un generador de reglas en GPU aprovechando la aceleración por hardware y el procesamiento paralelo para conseguir una generación de reglas en la mitad de tiempo que SparseConvNet. Este extractor es utilizado para convertir la información 3D en un formato similar a una imagen \ac{bev}.
\item \acl{rpn}\par
Las \acs{rpn} fueron presentadas junto con el detector Fast R-CNN \cite{fast_rcnn} y son utilizadas en este modelo para que a partir del mapa de características extraído de la fase anterior, obtenga las predicciones del modelo.
\item Obtención de las detecciones\par
En la salida se utilizan unos tamaños fijados para las diferentes clases que son ajustados a las detecciones, a partir del centro del objeto detectado. Por cada objeto es fijado un one-hot vector para el ajuste de las cajas y otro para el ajuste de la dirección. Tras esto se aplica un límite de precisión por clase para minimizar los falsos positivos.
\end{enumerate}

Este método tras su publicación consiguió convertirse en \acs{sota} en el benchmark KITTI en su evaluador de detección utilizando \acs{lidar}, además de realizar esto en tiempo real, con tiempos de computo de 20 Hz en su modelo completo y 40 Hz en su modelo reducido.

\subsection{PointPillars}
\label{sec:pointpillars}

PointPillars se publica meses después de SECOND y no solo consigue una mejora en la precisión para la detección de objetos 3D sino que consigue esto a una velocidad de inferencia de hasta 62 Hz o hasta 105 Hz con su modelo reducido. Esto no solo permite la utilización con nubes de puntos mayores sin disminuir apenas el rendimiento sino que permite la integración de este modelo en sistemas embebidos.\par
Para la obtención de esta velocidad de inferencia, PointPillars elimina el uso de las capas convolucionales 3D, convirtiendo las nubes de puntos en imágenes \acs{bev} de la escena. Los principales componentes de la red que consiguen este funcionamiento son los siguientes:

\begin{figure}[H]
	\centering
	\includegraphics[width=1\textwidth]{figures/4_sistemas_de_percepcion_con_lidar_basados_en_deep_learning/pointpillars.png}
	\caption{Arquitectura propuesta del detector PointPillars.}
	\label{fig:Arquitectura_propuesta_del_detector_pointpillars}
\end{figure}

\newpage

\begin{enumerate}
\item Nube de puntos a pseudo imágenes\par
Se comienza voxelizando la nube de puntos formando diferentes pilares, esto quiere decir que cada vóxel tiene un tamaño $x$ e $y$ fijo pero en $z$ se tiene un tamaño infinito. Por cada punto en los vóxeles se les añade junto a las tres dimensiones con la información de posición: el grado de reflectividad, la distancia aritmética a la media de los puntos del vóxel en las tres dimensiones y el desplazamiento en $x$ e $y$ al centro del vóxel, por lo que se tiene un espacio de nueve dimensiones por pilar. Para limitar el cómputo, se define un támaño máximo por pilar de [9, número de pilares no vacíos, número de puntos por pilar], se esta manera se fija un máximo de números de puntos por pilar y se aplica zero padding si el pilar se encuentra con muy poca información.\par
Tras esto se aplica una capa linear con batch normalization y una \acs{relu} seguido de una operación de máximo por cada uno de los puntos del pilar para obtener un tensor de tamaño [X, número de pilares no vacíos], lo cual es pasado a una pseudo imagen pasando el número de pilares no vacíos a la altura y anchura de la imagen en función de la posición de dichos pilares. Con lo que se consigue un estructura similar a la de una imagen \acs{bev}.
\item Backbone\par
Se utiliza un backbone similar al de VoxelNet \cite{voxelnet}, este backbone tiene dos subcapas, una que aumenta el número de características a nivel espacial y otra que aumenta y relaciona las características de los pilares.
\item Cabeza detectora\par
En la salida se utiliza una \ac{ssd} para obtener las salidas de las detecciones 3D.
\end{enumerate}

Este enfoque de uso de \acs{cnn} sobre nubes de puntos de la misma manera que se analizaría para imágenes, se ha visto en este paper, que es muy útil para acelerar la inferencia del modelo, además que se consigue un muy buen rendimiento para tareas de detección 3D y detección \acs{bev} 2D.

\subsection{PointRCNN}
\label{sec:pointrcnn}

PointRCNN \cite{pointrcnn} propone un modelo basado en dos fases para la detección de objetos 3D. Dichas fases consisten en una primera fase de generación de las detecciones 3D y otra de refinamiento de las bounding boxes 3D.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{figures/4_sistemas_de_percepcion_con_lidar_basados_en_deep_learning/pointrcnn.png}
	\caption{Arquitectura propuesta del detector PointRCNN.}
	\label{fig:Arquitectura_propuesta_del_detector_pointrcnn}
\end{figure}

El funcionamiento de las dos fases del modelo es el siguiente:

\begin{enumerate}
\item Generación de propuestas de detección 3D vía segmentación de la nube de puntos\par
La primera fase del modelo se basa en el backbone de PointNet++ \cite{pointnet++} para extraer las características a nivel de punto de la nube de puntos utilizando un agrupación multiescala. Con dicho modelo junto con un método propio basado en la discretización 2D en \acs{bev} (bin-based) se hayan las múltiples detecciones 3D, pero se reducen por cada objeto aplicando \ac{nms} basado en el \ac{iou} como \acs{bev} con un límite de 0,8 y solo las 100 mejores detecciones son mantenidas para la siguiente fase de ajuste de las detecciones 3D.
\item Refinamiento de las bounding boxes 3D\par
Tras la obtención de las bounding boxes 3D, se trata de mejorar el centro y la orientación de estas. Por cada una de dichas detecciones se aumenta su tamaño por un valor constante incluyendo además una mascara que diferencia aquellos puntos de la detección original al espacio aumentado. Cada una de dichas detecciones aumentadas pasan a utilizar el sistema de coordenadas propio de cada detección.
\begin{figure}[H]
	\centering
	\includegraphics[width=0.6\textwidth]{figures/4_sistemas_de_percepcion_con_lidar_basados_en_deep_learning/region_pooling_pointrcnn.png}
	\caption{Agrupación de regiones de la nube de puntos en el modelo PointRCNN.}
	\label{fig:Agrupacion_de_regiones_de_la_nube_de_puntos_en_el_modelo_pointrcnn}
\end{figure}
A partir de los puntos de cada agrupación de las características extraídas de la primera fase y de un parámetro que agrega información de la distancia al sensor para aquellas agrupaciones con menos puntos calculado como $\sqrt{x²+y²+z²}$, se introducen en múltiples capas \acs{fc} para obtener las características locales. Tras esto se vuelve a utilizar el modelo bin-based utilizado en la primera fase con lo que se obtienen las bounding boxes 3D aplicando nuevamente \acs{nms} sobre un \acs{iou} en \acs{bev} con un límite de 0,01 para eliminar solapamientos.
\end{enumerate}

Al encontrarse basado este método en dos subredes, sufre de un tiempo de computo mayor, lo que se traduce en una velocidad de 12 Hz \cite{rangercnn}, pero suficiente para aplicarse en tiempo real al obtener típicamente las nubes de puntos a 10 Hz.

\subsection{PV-RCNN}
\label{sec:pv_rcnn}

PV-RCNN \cite{pv_rcnn} unifica los beneficios de las dos principales técnicas de detección de objetos utilizando \acs{lidar}, como son: el uso de técnicas de voxelización junto con Sparse Convolutional Networks y el uso de backbones como el de PointNet \cite{pointnet} o métodos similares. Para ello se construye un modelo basado en tres pasos:

\newpage

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{figures/4_sistemas_de_percepcion_con_lidar_basados_en_deep_learning/pvrcnn.png}
	\caption{Arquitectura propuesta del detector PV-RCNN.}
	\label{fig:Arquitectura_propuesta_del_detector_pv_rcnn}
\end{figure}

\begin{enumerate}
\item 3D Voxel \acs{cnn} para la extracción de características y generación de detecciones\par
Se utiliza una Voxel \acs{cnn} con 3D sparse convolution junto con una \acs{rpn}, la cual es una elección popular en el \acs{sota} gracias a su eficiencia, por lo que es el backbone utilizado utilizado en este modelo. Este método obtiene de forma interna características semánticas de los vóxeles además de conseguir las detecciones de los objetos a partir de los tamaños prefijados o anchors.
\item Codificación de escenas de vóxel a puntos clave\par
Primero se agregan las características de los vóxeles en un conjunto de puntos clave que sirven de conexión entre la fase anterior y el refinamiento de las detecciones. Para la obtención de los puntos clave se adopta el algoritmo Furthest Point Sampling y se aplica sobre la nube de puntos, lo cual elige puntos de forma uniformemente distribuida sobre los vóxeles no vacíos. Tras esto se propone el módulo Voxel Set Abstraction para codificar características semánticas de la primera fase en los puntos clave. Basándose en la premisa de que los puntos más cercanos tiene que contribuir más a la propuesta de detección se propone el módulo Predicted Keypoint Weighting.
\begin{figure}[H]
	\centering
	\includegraphics[width=0.5\textwidth]{figures/4_sistemas_de_percepcion_con_lidar_basados_en_deep_learning/pkw_pvrcnn.png}
	\caption{Módulo Predicted Keypoint Weighting del modelo PV-RCNN.}
	\label{fig:Agrupacion_de_regiones_de_la_nube_de_puntos_en_el_modelo_pc_rcnn}
\end{figure}
En dicho módulo se introducen los puntos clave codificados previamente para dar un peso a cada uno en función de su importancia para las detecciones.
\item Abstracción de características de \acs{roi} de puntos clave a cuadrícula para el refinamiento de la detección\par
Para el refinamiento de las detecciones se utiliza un método de abstracción de características \acs{roi} de puntos clave a rejilla. Dadas las detecciones junto con los puntos codificados y sus pesos, se agregan entorno a diferentes puntos de rejilla con un radio determinado. Tras esto se obtienen las características de las rejillas y a partir de estas, con una pequeña red de dos capas se ajusta la detección.
\end{enumerate}

De la misma manera que ocurría en PointRCNN, PV-RCNN tiene una velocidad de inferencia baja de 10 Hz \cite{rangercnn} debido a la fase de refinamiento y el procesamiento adicional de las características del backbone.

\subsection{CBGS}
\label{sec:cbgs}

Class-balanced Grouping and Sampling (CBGS) \cite{cbgs} propone un modelo compuesto principalmente de 4 partes: módulo de entrada, extractor de características 3D, \acs{rpn} y una red con múltiples cabeceras en función del grupo. Con esto se consigue un funcionamiento en detección 3D, predicción de la velocidad y de la clase, todo ello sobre las 10 clases diferentes que utiliza nuScenes.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{figures/4_sistemas_de_percepcion_con_lidar_basados_en_deep_learning/cbgs.png}
	\caption{Arquitectura propuesta del detector CBGS.}
	\label{fig:Arquitectura_propuesta_del_detector_pv_rcnn}
\end{figure}

Parte del funcionamiento del modelo proviene de la fase de entrenamiento, en la que se aplica una técnica de muestreo que trata de mejorar la media de uso de las diferentes clases para reducir la irregularidad del dataset de nuScenes, lo que aumenta el conjunto de entrenamiento de 28.130 a 128.100 muestras.\par
Para realizar la inferencia en el modelo se siguen los siguientes pasos:

\begin{enumerate}
\item Entrada de la red\par
Debido al método de validación de las detecciones en nuScenes es necesaria la inferencia de la velocidad por lo que siguiendo el método oficial del dataset, se utilizan diez barridos de \acs{lidar} para la obtención del dato de la velocidad. En la entrada del modelo es necesario entonces alguna medida de tiempo, por lo que cada punto se encuentra codificado como [$x$, $y$, $z$, $intensity$, $\Delta t$], siendo $\Delta t$ la diferencia de tiempo entre el barrido inicial y el barrido del que proviene el punto. Tras esto se voxeliza la nube de puntos con un tamaño de vóxel de [0.1, 0.1, 0.2] m para la reducción del tiempo de computo. Dentro de cada vóxel se mantiene unicamente la media de todos puntos de dicho vóxel.
\item Backbone utilizado\par
En la red principal es utilizado un modelo basado en sparse 3D convolution y un extractor de características. Tras esto se aplica un \acs{rpn}, lo que termina en una red similar a la de VoxelNet \cite{voxelnet} para extraer más características pero con capas convolucionales 2D.
\item Class-balance Grouping\par
Al tener una aparición de las diferentes clases muy irregular, con casi un 45\% de los objetos siendo coches, es muy difícil para objetos con formas diferentes extraer las diferencias entre ellos. Por ejemplo una bicicleta y una motocicleta tienen una forma muy similar en lo que respecta a la nube de puntos, al igual que ocurren con un camión y un camión de construcción. Para solucionar este problema se propone la agrupación de clases en función de su similitud, teniendo en cuenta el tamaño de los objetos y un balanceo del tamaño de los grupos en función del número de clases. Lo cual acaba generando 6 grupos diferentes: (Coche), (Camión, Camión de construcción), (Bus, Tráiler), (Barrera), (Motocicleta, Bicicleta), (Peatón, Cono de tráfico). Por lo que tras la elección del grupo, se obtiene su cabecera y se hayan la posición, tamaño, rotación y velocidad.
\end{enumerate}

Este método consigue por tanto una detección de objetos 3D además de un pseudo tracking para inferir la velocidad de los objetos, todo ello trabajando con 10 clases diferentes y obteniendo una velocidad de 9 Hz.

\section{OpenPCDet}
\label{sec:openpcdet}


